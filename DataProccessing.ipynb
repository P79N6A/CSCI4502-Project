{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using python libs cucco and unidecode as well as standard libs for text normalization\n",
    "from cucco import Cucco\n",
    "from html import unescape\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "cucco = Cucco()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Tesla Stocks\n",
    "dfStockData = pd.read_json(\"data/TSLA_stocks.json\").transpose()\n",
    "\n",
    "# Get difference of open and close prices\n",
    "dfStockData[\"diff\"] = dfStockData[\"4. close\"] - dfStockData[\"1. open\"]\n",
    "dfStockMin = dfStockData.drop(columns=[\"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\"])\n",
    "\n",
    "# Make date a regular field instead of an index\n",
    "dfStockMin.index.name = \"date\"\n",
    "dfStockMin.reset_index(inplace=True)\n",
    "\n",
    "# Making it a datetime\n",
    "dfStockMin[\"date\"] = pd.to_datetime(dfStockMin[\"date\"])\n",
    "\n",
    "# Do min/max normalization on diff values, to get values from -1 to 1\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "vals = dfStockMin[\"diff\"].values\n",
    "vals = scaler.fit_transform(vals.reshape(-1,1))\n",
    "dfStockMin[\"diff\"] = vals\n",
    "\n",
    "# Turn stock data into a csv\n",
    "dfStockMin.to_csv(\"normed_stockdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizations = [\n",
    "    \"replace_emails\",\n",
    "    \"replace_urls\",\n",
    "    \"remove_stop_words\",\n",
    "    \"replace_emojis\",\n",
    "    \"replace_symbols\",\n",
    "    \"replace_hyphens\",\n",
    "    \"replace_punctuation\",\n",
    "    \"remove_extra_white_spaces\"\n",
    "]\n",
    "def normalize_str(s):\n",
    "    s = unidecode(unescape(s))\n",
    "    # remove drop words, stock symbols, 1-2 letter words, and numbers\n",
    "    s = remove_regex.sub(\"\", s)\n",
    "    s = cucco.normalize(s, normalizations).strip().lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tweet data\n",
    "dfTweet = pd.DataFrame()\n",
    "for i in range(11275):\n",
    "    df = json.load(open(\"data/messages/resp_%s.json\" % i))\n",
    "    dftemp = json_normalize(df['messages'])\n",
    "    dfTweet = dfTweet.append(dftemp[['body', 'created_at']], ignore_index=True)\n",
    "    \n",
    "# Create bag of words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(dfTweet['body']).todense()\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "# Turn tweet data into a csv and read it in\n",
    "dfTweet.to_csv(\"clean_tweet_data.csv\")\n",
    "df = pd.read_csv(\"clean_tweet_data.csv\")\n",
    "\n",
    "# Read in clean tweet data\n",
    "dfTweet = pd.read_csv(\"clean_tweet_data.csv\")\n",
    "\n",
    "# Convert created_at to just dates without time\n",
    "dfTweet[\"date\"] = dfTweet[\"created_at\"].str.split('T',expand=True)[0]\n",
    "dfTweet[\"date\"] = pd.to_datetime(dfTweet[\"date\"])\n",
    "dfTweet = dfTweet.drop(columns=\"created_at\")\n",
    "dfTweet = dfTweet.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "drop_words = [\"tsla\"]\n",
    "\n",
    "remove_regex = re.compile(r\"\\b(?:{}|$\\w+|\\w\\w?|\\d+)\\b\".format(\"|\".join(re.escape(w) for w in drop_words)), flags=re.IGNORECASE)\n",
    "\n",
    "dfTweet[\"body\"] = dfTweet[\"body\"].apply(normalize_str)\n",
    "\n",
    "# Remove empty rows\n",
    "dfTweet = dfTweet[dfTweet[\"body\"].map(len) > 0]\n",
    "\n",
    "dfTweet.to_csv(\"normed_tweets.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTweet = pd.read_csv(\"normed_tweets.csv\")\n",
    "dfStock = pd.read_csv(\"normed_stockdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get counts for words\n",
    "counts = Counter()\n",
    "for tweet in dfTweet[\"body\"]:\n",
    "    words = tweet.split(\" \")\n",
    "    for word in words:\n",
    "        counts[word] += 1\n",
    "\n",
    "# Now assign each word an index, and eliminate words with less than 50 occurrences\n",
    "# as well as the most frequent words\n",
    "most_frequent = [w for w,c in counts.most_common(9)]\n",
    "indices = {}\n",
    "i = 0\n",
    "for k in counts:\n",
    "    if counts[k] >= 50 and k not in most_frequent:\n",
    "        indices[k] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now create word vectors, normalizing from 0-1 for each individually\n",
    "# Each word vector will represent one day's worth of tweets.\n",
    "# Also put the correct output data for the corresponding date\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "input_data = []\n",
    "output_data = []\n",
    "for date in set(dfTweet[\"date\"]):\n",
    "    stock = dfStock[dfStock[\"date\"] == date][\"diff\"]\n",
    "    if len(stock) > 0:\n",
    "        output_data.append(stock.iloc[0])\n",
    "        \n",
    "        word_vec = [0.0] * len(indices)\n",
    "        tweets = dfTweet[dfTweet[\"date\"] == date][\"body\"]\n",
    "        for tweet in tweets:\n",
    "            words = tweet.split(\" \")\n",
    "            for word in words:\n",
    "                if word in indices:\n",
    "                    word_vec[indices[word]] += 1\n",
    "        word_vec = scaler.fit_transform(np.reshape(word_vec, (-1,1)))\n",
    "        input_data.append(word_vec.reshape(1,-1)[0])\n",
    "\n",
    "# Now split the data so that we have 20% left to evaluate accuracy\n",
    "split_point = int(len(output_data) * 0.8)\n",
    "input_train = input_data[:split_point]\n",
    "output_train = output_data[:split_point]\n",
    "input_test = input_data[split_point:]\n",
    "output_test = output_data[split_point:]\n",
    "input_data = None\n",
    "output_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(2268, 67), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(solver=\"adam\", alpha=1e-5, hidden_layer_sizes=(len(indices)//2,int(np.sqrt(len(indices)))))\n",
    "mlp.fit(input_train, output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2080 -0.0820  0.1259\n",
      "-0.1114 -0.0713  0.0401\n",
      " 0.1964  0.0374  0.1590\n",
      "-0.1559  0.1107  0.2666\n",
      "-0.1803 -0.1833  0.0031\n",
      "-0.1682 -0.0879  0.0803\n",
      " 0.1443 -0.1130  0.2573\n",
      "-0.2838 -0.0563  0.2275\n",
      " 0.0111 -0.3218  0.3330\n",
      "-0.0726 -0.1162  0.0436\n",
      "-0.1939 -0.1185  0.0754\n",
      " 0.2340  0.2233  0.0107\n",
      "-0.0534 -0.1052  0.0517\n",
      "-0.0898 -0.0352  0.0546\n",
      "-0.1895 -0.2058  0.0164\n",
      " 0.0597  0.0658  0.0061\n",
      "-0.0889 -0.2469  0.1581\n",
      "-0.3872 -0.5693  0.1821\n",
      "-0.0368 -0.0708  0.0340\n",
      "-0.1305 -0.1597  0.0292\n",
      "-0.3515 -0.3748  0.0233\n",
      "-0.1086  0.1474  0.2560\n",
      "-0.0168 -0.1811  0.1644\n",
      "-0.0801 -0.2168  0.1368\n",
      "-0.1976 -0.3342  0.1366\n",
      " 0.0346 -0.3516  0.3862\n",
      "-0.0801 -0.1350  0.0549\n",
      "-0.1356 -0.0357  0.0999\n",
      " 0.0769 -0.2840  0.3610\n",
      "-0.5151 -0.1956  0.3195\n",
      "-0.4452 -0.1554  0.2898\n",
      " 0.2277  0.0187  0.2090\n",
      " 0.5415  0.1515  0.3899\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(input_test)\n",
    "for a,b in zip(output_test, predictions):\n",
    "    print(\"{:7.4f}\".format(a),\"{:7.4f}\".format(b),\"{:7.4f}\".format(abs(b-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19287355282456295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(output_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the NN and training/test data to file\n",
    "with open(\"indices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(indices, f)\n",
    "with open(\"orig_x_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_train, f)\n",
    "with open(\"orig_y_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_train, f)\n",
    "with open(\"orig_x_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_test, f)\n",
    "with open(\"orig_y_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_test, f)\n",
    "with open(\"original_nn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the indices for word vectors\n",
    "with open(\"indices.pkl\", \"rb\") as f:\n",
    "    indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the training data\n",
    "with open(\"orig_x_train.pkl\", \"rb\") as f:\n",
    "    input_train = pickle.load(f)\n",
    "with open(\"orig_y_train.pkl\", \"rb\") as f:\n",
    "    output_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the test data\n",
    "with open(\"orig_x_test.pkl\", \"rb\") as f:\n",
    "    input_test = pickle.load(f)\n",
    "with open(\"orig_y_test.pkl\", \"rb\") as f:\n",
    "    output_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the NN\n",
    "with open(\"original_nn.pkl\", \"rb\") as f:\n",
    "    mlp = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
