{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[0., 0.], [1., 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[-5, 1], [-13., -2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in clf.coefs_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-05</th>\n",
       "      <td>340.50</td>\n",
       "      <td>343.9550</td>\n",
       "      <td>330.140</td>\n",
       "      <td>341.40</td>\n",
       "      <td>7829954.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>343.74</td>\n",
       "      <td>349.2000</td>\n",
       "      <td>340.910</td>\n",
       "      <td>346.41</td>\n",
       "      <td>7807971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>338.26</td>\n",
       "      <td>347.8400</td>\n",
       "      <td>334.725</td>\n",
       "      <td>344.28</td>\n",
       "      <td>8000132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>332.54</td>\n",
       "      <td>342.0000</td>\n",
       "      <td>329.100</td>\n",
       "      <td>337.32</td>\n",
       "      <td>7624348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>328.39</td>\n",
       "      <td>337.8999</td>\n",
       "      <td>322.260</td>\n",
       "      <td>329.90</td>\n",
       "      <td>9126704.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1. open   2. high   3. low  4. close  5. volume\n",
       "2018-11-05   340.50  343.9550  330.140    341.40  7829954.0\n",
       "2018-11-02   343.74  349.2000  340.910    346.41  7807971.0\n",
       "2018-11-01   338.26  347.8400  334.725    344.28  8000132.0\n",
       "2018-10-31   332.54  342.0000  329.100    337.32  7624348.0\n",
       "2018-10-30   328.39  337.8999  322.260    329.90  9126704.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStockData = pd.read_json(\"data/TSLA_stocks.json\").transpose()\n",
    "dfStockData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body  \\\n",
      "0       $TSLA very small put add today. Waiting for th...   \n",
      "1                   $TSLA target 370 for the short Again?   \n",
      "2               Whole new company now shorts beware $TSLA   \n",
      "3              $TSLA Scary scary üêª TSLA‚Äôs coming for you!   \n",
      "4       $TSLA max pain is 315 for expiry 2018-11-02 So...   \n",
      "5       $TSLA Happy Halloween all. And this one is ded...   \n",
      "6       $TSLA the action this week builds confidence. ...   \n",
      "7       $TSLA AUTO PARK!! yes!!! When I gotta grab a q...   \n",
      "8       $TSLA Charts show 344 is where this maxes out ...   \n",
      "9                         $TSLA I love your kinda crazy üòú   \n",
      "10      $TSLA jumps to #1 in global plug-in electric c...   \n",
      "11      10-31-2018 Potential Whale Trades for $TSLA (L...   \n",
      "12                       $TSLA chairmen announcement soon   \n",
      "13      $TSLA there is no truth to the rumor I share t...   \n",
      "14      $TSLA everyone want&#39;s the treat but will i...   \n",
      "15      $TSLA will need some strong fresh news to brea...   \n",
      "16      $TSLA out of the darkness (QQQ -9%) a new üëë sh...   \n",
      "17      $TSLA üéÉ Highest ever stock price on  Halloween! üéÉ   \n",
      "18      $TSLA is a safe place to park your $$$ but $AM...   \n",
      "19      $tsla When do i believe this correction will e...   \n",
      "20      $TSLA Starting a list of usernames to taunt wh...   \n",
      "21                                  $SOLO $NIO $TSLA $TTM   \n",
      "22                     $TSLA buying more on any pull back   \n",
      "23      Peak gain % for last 5 expired setups $TSLA 30...   \n",
      "24      $TSLA imagine that fb files their 10Q the day ...   \n",
      "25      @eclass @blackcherry Literally, Tesla held me ...   \n",
      "26      $TSLA wonder why all the big purchases at SP 3...   \n",
      "27      $TSLA shorts are TOAST! yet another short sell...   \n",
      "28      $TSLA \\n\\nTesla is way too undervalued.\\n\\nüëáüèª ...   \n",
      "29      $TSLA Remember the only one you are fighting a...   \n",
      "...                                                   ...   \n",
      "339439              $TSLA suspends production is not good   \n",
      "339440  $TSLA https://www.youtube.com/watch?v=tls-k39rrUE   \n",
      "339441  $TSLA Elon made a fool out of himself this wee...   \n",
      "339442  $TSLA Kimble Musk is raffling off a fully load...   \n",
      "339443                                   $TSLA jumps tomm   \n",
      "339444  $TSLA   \\nElon Musk, speaking at SXSW, project...   \n",
      "339445  good spot, thanks @mrkd ... though when he lau...   \n",
      "339446  $TSLA haha you&#39;re telling me. not sure if ...   \n",
      "339447  WeeklyAlgoWatchlist# March11,2018: goo.gl/AqWC...   \n",
      "339448  $TSLA brown-nose much? https://www.engadget.co...   \n",
      "339449  $FCAU $TSLA https://www.bbc.co.uk/news/amp/bus...   \n",
      "339450  $TSLA production offline for five days in late...   \n",
      "339451  RT $TSLA   http://maximum-pain.com/twitter/fol...   \n",
      "339452  $TSLA is one of the better performing stocks i...   \n",
      "339453  Estimize revenue expectations are 4.49% higher...   \n",
      "339454  $AAPL $TSLA $MU $MSFT $IBM $GE $NFLX $AMZN $FB...   \n",
      "339455  $TSLA , Final paragraph of Bloomberg productio...   \n",
      "339456  $TSLA #Tesla http://quad7capital.com/2018/03/1...   \n",
      "339457  $TSLA , I really don&#39;t think Felon Muck he...   \n",
      "339458  My article Why I Trade Options http://bit.ly/2...   \n",
      "339459  Weekly Recap $QQQ $SPY $MU $INTC $TSLA $NFLX $...   \n",
      "339460  $TSLA seems like a decent resource to keep tra...   \n",
      "339461  $TSLA @mrkd This Tesla shorter is clueless. Ca...   \n",
      "339462                    $TSLA Waiting for $290&#39;s...   \n",
      "339463                 $TSLA https://youtu.be/5TMnH4foxM0   \n",
      "339464  Here‚Äôs what 37 Estimize analysts believe $TSLA...   \n",
      "339465  $TSLA $SPY looking at him gets me high! Dude i...   \n",
      "339466  $TSLA Insiders adding shares recntly, anyways ...   \n",
      "339467  $TSLA not looking great. $321, $317 take it to...   \n",
      "339468  $TSLA been in this range for almost a year. RS...   \n",
      "\n",
      "                  created_at  \n",
      "0       2018-10-31T21:20:53Z  \n",
      "1       2018-10-31T21:20:53Z  \n",
      "2       2018-10-31T21:19:49Z  \n",
      "3       2018-10-31T21:16:54Z  \n",
      "4       2018-10-31T21:15:52Z  \n",
      "5       2018-10-31T21:14:07Z  \n",
      "6       2018-10-31T21:13:02Z  \n",
      "7       2018-10-31T21:12:41Z  \n",
      "8       2018-10-31T21:09:22Z  \n",
      "9       2018-10-31T21:08:24Z  \n",
      "10      2018-10-31T21:04:01Z  \n",
      "11      2018-10-31T21:03:35Z  \n",
      "12      2018-10-31T21:00:50Z  \n",
      "13      2018-10-31T20:59:21Z  \n",
      "14      2018-10-31T20:59:18Z  \n",
      "15      2018-10-31T20:58:44Z  \n",
      "16      2018-10-31T20:58:05Z  \n",
      "17      2018-10-31T20:56:31Z  \n",
      "18      2018-10-31T20:56:20Z  \n",
      "19      2018-10-31T20:55:10Z  \n",
      "20      2018-10-31T20:54:16Z  \n",
      "21      2018-10-31T20:51:49Z  \n",
      "22      2018-10-31T20:51:39Z  \n",
      "23      2018-10-31T20:47:33Z  \n",
      "24      2018-10-31T20:42:54Z  \n",
      "25      2018-10-31T20:42:27Z  \n",
      "26      2018-10-31T20:41:41Z  \n",
      "27      2018-10-31T20:41:36Z  \n",
      "28      2018-10-31T20:41:07Z  \n",
      "29      2018-10-31T20:40:04Z  \n",
      "...                      ...  \n",
      "339439  2018-03-12T02:32:14Z  \n",
      "339440  2018-03-12T02:30:26Z  \n",
      "339441  2018-03-12T02:24:37Z  \n",
      "339442  2018-03-12T02:21:10Z  \n",
      "339443  2018-03-12T02:20:08Z  \n",
      "339444  2018-03-12T02:19:42Z  \n",
      "339445  2018-03-12T02:16:33Z  \n",
      "339446  2018-03-12T02:14:05Z  \n",
      "339447  2018-03-12T01:39:25Z  \n",
      "339448  2018-03-12T01:25:15Z  \n",
      "339449  2018-03-12T01:16:41Z  \n",
      "339450  2018-03-12T01:00:56Z  \n",
      "339451  2018-03-12T00:57:25Z  \n",
      "339452  2018-03-12T00:53:04Z  \n",
      "339453  2018-03-12T00:40:56Z  \n",
      "339454  2018-03-12T00:37:39Z  \n",
      "339455  2018-03-12T00:35:43Z  \n",
      "339456  2018-03-12T00:33:41Z  \n",
      "339457  2018-03-12T00:32:14Z  \n",
      "339458  2018-03-12T00:31:50Z  \n",
      "339459  2018-03-12T00:23:43Z  \n",
      "339460  2018-03-12T00:21:42Z  \n",
      "339461  2018-03-12T00:13:35Z  \n",
      "339462  2018-03-12T00:00:28Z  \n",
      "339463  2018-03-11T23:36:12Z  \n",
      "339464  2018-03-11T23:28:45Z  \n",
      "339465  2018-03-11T23:17:02Z  \n",
      "339466  2018-03-11T23:11:54Z  \n",
      "339467  2018-03-11T22:57:19Z  \n",
      "339468  2018-03-11T22:54:18Z  \n",
      "\n",
      "[339469 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "for i in range(11275):\n",
    "    df = json.load(open(\"data/messages/resp_%s.json\" % i))\n",
    "    dftemp = json_normalize(df['messages'])\n",
    "    dfTweet = dfTweet.append(dftemp[['body', 'created_at']], ignore_index=True)\n",
    "print(dfTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(dfTweet['body']).todense()\n",
    "vectorizer.vocabulary_\n",
    "print(max(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet.to_csv(\"clean_tweet_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339464</th>\n",
       "      <td>339464</td>\n",
       "      <td>Here‚Äôs what 37 Estimize analysts believe $TSLA...</td>\n",
       "      <td>2018-03-11T23:28:45Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339465</th>\n",
       "      <td>339465</td>\n",
       "      <td>$TSLA $SPY looking at him gets me high! Dude i...</td>\n",
       "      <td>2018-03-11T23:17:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339466</th>\n",
       "      <td>339466</td>\n",
       "      <td>$TSLA Insiders adding shares recntly, anyways ...</td>\n",
       "      <td>2018-03-11T23:11:54Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339467</th>\n",
       "      <td>339467</td>\n",
       "      <td>$TSLA not looking great. $321, $317 take it to...</td>\n",
       "      <td>2018-03-11T22:57:19Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339468</th>\n",
       "      <td>339468</td>\n",
       "      <td>$TSLA been in this range for almost a year. RS...</td>\n",
       "      <td>2018-03-11T22:54:18Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               body  \\\n",
       "339464      339464  Here‚Äôs what 37 Estimize analysts believe $TSLA...   \n",
       "339465      339465  $TSLA $SPY looking at him gets me high! Dude i...   \n",
       "339466      339466  $TSLA Insiders adding shares recntly, anyways ...   \n",
       "339467      339467  $TSLA not looking great. $321, $317 take it to...   \n",
       "339468      339468  $TSLA been in this range for almost a year. RS...   \n",
       "\n",
       "                  created_at  \n",
       "339464  2018-03-11T23:28:45Z  \n",
       "339465  2018-03-11T23:17:02Z  \n",
       "339466  2018-03-11T23:11:54Z  \n",
       "339467  2018-03-11T22:57:19Z  \n",
       "339468  2018-03-11T22:54:18Z  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"clean_tweet_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>body</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339464</th>\n",
       "      <td>339464</td>\n",
       "      <td>Here‚Äôs what 37 Estimize analysts believe $TSLA...</td>\n",
       "      <td>2018-03-11T23:28:45Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339465</th>\n",
       "      <td>339465</td>\n",
       "      <td>$TSLA $SPY looking at him gets me high! Dude i...</td>\n",
       "      <td>2018-03-11T23:17:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339466</th>\n",
       "      <td>339466</td>\n",
       "      <td>$TSLA Insiders adding shares recntly, anyways ...</td>\n",
       "      <td>2018-03-11T23:11:54Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339467</th>\n",
       "      <td>339467</td>\n",
       "      <td>$TSLA not looking great. $321, $317 take it to...</td>\n",
       "      <td>2018-03-11T22:57:19Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339468</th>\n",
       "      <td>339468</td>\n",
       "      <td>$TSLA been in this range for almost a year. RS...</td>\n",
       "      <td>2018-03-11T22:54:18Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               body  \\\n",
       "339464      339464  Here‚Äôs what 37 Estimize analysts believe $TSLA...   \n",
       "339465      339465  $TSLA $SPY looking at him gets me high! Dude i...   \n",
       "339466      339466  $TSLA Insiders adding shares recntly, anyways ...   \n",
       "339467      339467  $TSLA not looking great. $321, $317 take it to...   \n",
       "339468      339468  $TSLA been in this range for almost a year. RS...   \n",
       "\n",
       "                  created_at  \n",
       "339464  2018-03-11T23:28:45Z  \n",
       "339465  2018-03-11T23:17:02Z  \n",
       "339466  2018-03-11T23:11:54Z  \n",
       "339467  2018-03-11T22:57:19Z  \n",
       "339468  2018-03-11T22:54:18Z  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStockData[\"diff\"] = dfStockData[\"4. close\"] - dfStockData[\"1. open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStockMin = dfStockData.drop(columns=[\"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStockMin.index.name = \"date\"\n",
    "dfStockMin.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStockMin[\"date\"] = pd.to_datetime(dfStockMin[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do min/max normalization on diff values, to get values from -1 to 1\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "vals = dfStockMin[\"diff\"].values\n",
    "vals = scaler.fit_transform(vals.reshape(-1,1))\n",
    "dfStockMin[\"diff\"] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStockMin.to_csv(\"normed_stockdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using python libs cucco and unidecode as well as standard libs for text normalization\n",
    "from cucco import Cucco\n",
    "from html import unescape\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "cucco = Cucco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet = pd.read_csv(\"clean_tweet_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert created_at to just dates without time\n",
    "dfTweet[\"date\"] = dfTweet[\"created_at\"].str.split('T',expand=True)[0]\n",
    "dfTweet[\"date\"] = pd.to_datetime(dfTweet[\"date\"])\n",
    "dfTweet = dfTweet.drop(columns=\"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet = dfTweet.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_words = [\"tsla\"]\n",
    "remove_regex = re.compile(r\"\\b(?:{}|$\\w+|\\w\\w?|\\d+)\\b\".format(\"|\".join(re.escape(w) for w in drop_words)), flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizations = [\n",
    "    \"replace_emails\",\n",
    "    \"replace_urls\",\n",
    "    \"remove_stop_words\",\n",
    "    \"replace_emojis\",\n",
    "    \"replace_symbols\",\n",
    "    \"replace_hyphens\",\n",
    "    \"replace_punctuation\",\n",
    "    \"remove_extra_white_spaces\"\n",
    "]\n",
    "def normalize_str(s):\n",
    "    s = unidecode(unescape(s))\n",
    "    # remove drop words, stock symbols, 1-2 letter words, and numbers\n",
    "    s = remove_regex.sub(\"\", s)\n",
    "    s = cucco.normalize(s, normalizations).strip().lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet[\"body\"] = dfTweet[\"body\"].apply(normalize_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty rows\n",
    "dfTweet = dfTweet[dfTweet[\"body\"].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body       date\n",
      "0             small put add today waiting clowns possibly 2018-10-31\n",
      "1                                      target short again 2018-10-31\n",
      "2                                   company shorts beware 2018-10-31\n",
      "3                                  scary scary coming you 2018-10-31\n",
      "4                                  max pain expiry source 2018-10-31\n",
      "5       happy halloween all dedicated bearish friends ... 2018-10-31\n",
      "6                    action week builds confidence friday 2018-10-31\n",
      "7       auto park yes gotta grab quickie store car roa... 2018-10-31\n",
      "8                 charts show maxes week surprised higher 2018-10-31\n",
      "9                                        love kinda crazy 2018-10-31\n",
      "10               jumps global plug electric car sales ytd 2018-10-31\n",
      "11            potential whale trades largest trade volume 2018-10-31\n",
      "12                                  chairmen announcement 2018-10-31\n",
      "13      truth rumor share nuerolink 5pm bradley cooper... 2018-10-31\n",
      "14                   want treat trick end happy halloween 2018-10-31\n",
      "15      strong fresh news break seems semi pre orders ... 2018-10-31\n",
      "16                    darkness qqq rise star born buy bah 2018-10-31\n",
      "17                          highest stock price halloween 2018-10-31\n",
      "18                    safe place park amd place bang buck 2018-10-31\n",
      "19       correction end welp nuerolink weeksmonth pattern 2018-10-31\n",
      "20                 starting list usernames taunt over lol 2018-10-31\n",
      "21                                           solo nio ttm 2018-10-31\n",
      "22                                       buying pull back 2018-10-31\n",
      "23                               peak gain expired setups 2018-10-31\n",
      "24                         imagine files 10q day earnings 2018-10-31\n",
      "25      eclass blackcherry literally tesla held market... 2018-10-31\n",
      "26                                          big purchases 2018-10-31\n",
      "27      shorts toast short seller covering today sheer... 2018-10-31\n",
      "28                  tesla undervalued read understand why 2018-10-31\n",
      "29                                      remember fighting 2018-10-31\n",
      "...                                                   ...        ...\n",
      "339436                 can produce model schedulemars lol 2018-03-12\n",
      "339437  monday watch qqq spy uvxy tvix nvda nflx wynn ... 2018-03-12\n",
      "339438  hahahhaha life time warranty car high hill tom... 2018-03-12\n",
      "339439                           suspends production good 2018-03-12\n",
      "339441                             elon made fool weekend 2018-03-12\n",
      "339442  kimble musk raffling fully loaded model boulde... 2018-03-12\n",
      "339443                                         jumps tomm 2018-03-12\n",
      "339444  elon musk speaking sxsw projects mars spaceshi... 2018-03-12\n",
      "339445   good spot mrkd laughs acting roleand retail eats 2018-03-12\n",
      "339446  haha you telling you heard anon conspiracy dig... 2018-03-12\n",
      "339447  weeklyalgowatchlist march11 gooaqwcpv aapl goo... 2018-03-12\n",
      "339448                                    brown nose much 2018-03-12\n",
      "339449                                               fcau 2018-03-12\n",
      "339450              production offline days late february 2018-03-12\n",
      "339452  performing stocks motor vehicles motor vehicle... 2018-03-12\n",
      "339453  estimize revenue expectations higher wall stre... 2018-03-12\n",
      "339454  aapl msft ibm nflx amzn technical day traders ... 2018-03-12\n",
      "339455       final paragraph bloomberg production tracker 2018-03-12\n",
      "339456             tesla time book profits analyst brahhh 2018-03-12\n",
      "339457        don felon muck helped ramblings couple days 2018-03-12\n",
      "339458  article trade options positions stocks ntnx cs... 2018-03-12\n",
      "339459  weekly recap qqq spy intc nflx baba nvda googl... 2018-03-12\n",
      "339460                   decent resource track tesla bull 2018-03-12\n",
      "339461  mrkd tesla shorter clueless calls paid pumper ... 2018-03-12\n",
      "339462                                            waiting 2018-03-12\n",
      "339464       here estimize analysts report deliveries amc 2018-03-11\n",
      "339465  spy high dude stoned fly mars dig underground ... 2018-03-11\n",
      "339466  insiders adding shares recntly tesla easily me... 2018-03-11\n",
      "339467                                        great trust 2018-03-11\n",
      "339468         range year rsi coiling break lead big move 2018-03-11\n",
      "\n",
      "[324264 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet.to_csv(\"normed_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet = pd.read_csv(\"normed_tweets.csv\")\n",
    "dfStock = pd.read_csv(\"normed_stockdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for words\n",
    "counts = Counter()\n",
    "for tweet in dfTweet[\"body\"]:\n",
    "    words = tweet.split(\" \")\n",
    "    for word in words:\n",
    "        counts[word] += 1\n",
    "\n",
    "# Now assign each word an index, and eliminate words with less than 50 occurrences\n",
    "# as well as the most frequent words\n",
    "most_frequent = [w for w,c in counts.most_common(9)]\n",
    "indices = {}\n",
    "i = 0\n",
    "for k in counts:\n",
    "    if counts[k] >= 50 and k not in most_frequent:\n",
    "        indices[k] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create word vectors, normalizing from 0-1 for each individually\n",
    "# Each word vector will represent one day's worth of tweets.\n",
    "# Also put the correct output data for the corresponding date\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "input_data = []\n",
    "output_data = []\n",
    "for date in set(dfTweet[\"date\"]):\n",
    "    stock = dfStock[dfStock[\"date\"] == date][\"diff\"]\n",
    "    if len(stock) > 0:\n",
    "        output_data.append(stock.iloc[0])\n",
    "        \n",
    "        word_vec = [0.0] * len(indices)\n",
    "        tweets = dfTweet[dfTweet[\"date\"] == date][\"body\"]\n",
    "        for tweet in tweets:\n",
    "            words = tweet.split(\" \")\n",
    "            for word in words:\n",
    "                if word in indices:\n",
    "                    word_vec[indices[word]] += 1\n",
    "        word_vec = scaler.fit_transform(np.reshape(word_vec, (-1,1)))\n",
    "        input_data.append(word_vec.reshape(1,-1)[0])\n",
    "\n",
    "# Now split the data so that we have 20% left to evaluate accuracy\n",
    "split_point = int(len(output_data) * 0.8)\n",
    "input_train = input_data[:split_point]\n",
    "output_train = output_data[:split_point]\n",
    "input_test = input_data[split_point:]\n",
    "output_test = output_data[split_point:]\n",
    "input_data = None\n",
    "output_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(2274, 67), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(solver=\"adam\", alpha=1e-5, hidden_layer_sizes=(len(indices)//2,int(np.sqrt(len(indices)))))\n",
    "mlp.fit(input_train, output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.4098  0.2075  0.2023\n",
      "-0.2120 -0.3241  0.1121\n",
      "-0.4452 -0.2109  0.2343\n",
      "-0.2678  0.3116  0.5794\n",
      " 0.0820 -0.0143  0.0962\n",
      " 0.0769 -0.2351  0.3120\n",
      "-0.0306  0.0137  0.0442\n",
      "-0.0801  0.0011  0.0812\n",
      "-0.6841 -0.2402  0.4439\n",
      "-0.7746 -0.4115  0.3631\n",
      "-0.0807  0.2902  0.3709\n",
      "-0.2838  0.0214  0.3052\n",
      "-0.2164 -0.0580  0.1584\n",
      "-0.3622 -0.0882  0.2739\n",
      "-0.0815 -0.1789  0.0974\n",
      "-0.4587  0.0396  0.4984\n",
      "-0.1732 -0.0603  0.1129\n",
      " 0.5900  0.0086  0.5814\n",
      "-0.4875  0.1172  0.6048\n",
      "-0.2700 -0.2511  0.0189\n",
      " 0.0845 -0.0450  0.1294\n",
      "-0.0773 -0.1317  0.0544\n",
      "-0.0958  0.1610  0.2568\n",
      " 0.1622 -0.1451  0.3073\n",
      "-0.1804  0.0629  0.2433\n",
      " 0.1653  0.0404  0.1249\n",
      "-0.0788 -0.2256  0.1468\n",
      "-0.3681 -0.1635  0.2047\n",
      "-0.1895 -0.2280  0.0385\n",
      "-0.5305  0.2215  0.7520\n",
      " 0.0575 -0.1360  0.1935\n",
      "-0.2848 -0.3315  0.0468\n",
      "-0.2384 -0.2246  0.0137\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(input_test)\n",
    "for a,b in zip(output_test, predictions):\n",
    "    print(\"{:7.4f}\".format(a),\"{:7.4f}\".format(b),\"{:7.4f}\".format(abs(b-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3074249035801162"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(output_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the NN and training/test data to file\n",
    "with open(\"indices.pkl\", \"wb\") as f:\n",
    "    pickle.dump(indices, f)\n",
    "with open(\"orig_x_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_train, f)\n",
    "with open(\"orig_y_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_train, f)\n",
    "with open(\"orig_x_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_test, f)\n",
    "with open(\"orig_y_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_test, f)\n",
    "with open(\"original_nn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the indices for word vectors\n",
    "with open(\"indices.pkl\", \"rb\") as f:\n",
    "    indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training data\n",
    "with open(\"orig_x_train.pkl\", \"rb\") as f:\n",
    "    input_train = pickle.load(f)\n",
    "with open(\"orig_y_train.pkl\", \"rb\") as f:\n",
    "    output_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the test data\n",
    "with open(\"orig_x_test.pkl\", \"rb\") as f:\n",
    "    input_test = pickle.load(f)\n",
    "with open(\"orig_y_test.pkl\", \"rb\") as f:\n",
    "    output_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the NN\n",
    "with open(\"original_nn.pkl\", \"rb\") as f:\n",
    "    mlp = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
